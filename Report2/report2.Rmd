---
title: "SDS_323 Exercise#2"
author: Yunlei Lu
date: Mar.30, 2020
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
```
```{r library}
library(tidyverse)
library(mosaic)
library(FNN)
library(foreach)
library(corrplot)
library(glmnet)
library(gamlr)
```
```{r RMSE}
rmse = function(y, y_pred) 
{
  sqrt(mean(data.matrix((y-y_pred)^2)))
}
```
```{r load data, include=FALSE}
path = "C:/Users/thinkpad/Desktop/FOCUS/UT_Course/SDS 323/exercise_2/data/sclass.csv"
sclass = read.csv(path)
data(SaratogaHouses)


# we focus on 2 trim levels: 350 and 65 AMG.
sclass_350 = subset(sclass, trim=='350')
summary(sclass_350)
sclass_65AMG = subset(sclass, trim=='65 AMG')
summary(sclass_65AMG)
```


# SDS323 Homework #2

## 1.KNN Regrssion for Car Price
     
### Objective
The data in [sclass.csv](../data/sclass.csv) contains data on over 29,000 Mercedes S Class vehicles.  
We focus on three variables in particular:
* trim: categorical variable for car's trim level, e.g. 350, 63 AMG, etc. The trim is like a sub-model designation.
* mileage: mileage on the car.
* price: the sales price in dollars of the car.

Our goal is to use K-nearest neighbors to build a predictive model for price, given mileage, separately for each of two trim levels: 350 and 65AMG.  



### Data
#### *Trim: S Class 350*
```{r plot origin 350}
ggplot(data=sclass_350) +
  geom_point(aes(x=mileage, y=price), size=1.5, color='#46ACC9', alpha=0.8) +
  theme_bw() +
  xlab('Mileage (km)') + ylab('Price ($)') +
  scale_x_continuous(breaks= seq(0, 150000, by=50000), labels=scales::comma) +
  scale_y_continuous(breaks= seq(0, 100000, by=20000), labels=scales::comma) +
  ggtitle('Price vs. Mileage') +
  labs(subtitle='S Class - 350')
```
###### Figure 1: Scatter plot of price against mileage for Mercedes S Class 350 vehicles


#### *Trim: S Class 65AMG*
```{r origin 65AMG}
ggplot(data=sclass_65AMG) +
  geom_point(aes(x=mileage, y=price), size=1.5, color='#46ACC9', alpha=0.8) +
  theme_bw() +
  xlab('Mileage (km)') + ylab('Price ($)') +
  scale_x_continuous(breaks= seq(0, 150000, by=50000), labels=scales::comma) +
  scale_y_continuous(breaks= seq(0, 250000, by=50000), labels=scales::comma) +
  ggtitle('Price vs. Mileage') +
  labs(subtitle='S Class - 65AMG')
```
###### Figure 2: Scatter plot of price against mileage for Mercedes S Class 65AMG vehicles



### Method
#### KNN Rrgression
We fit a KNN regression model to predict price given mileage.
In KNN regression, the KNN algorithm is used for estimating continuous variables. The value is the average of the values of k nearest neighbors. Here, k is the hyperparameter in the KNN regression model.
Then, we find a heuristically optimal number k of nearest neighbors, based on RMSE (Root Mean Square Error), this is done using repitition of random train-test split.



### Result

#### *Trim: S Class 350*
```{r }
# trian-test split
n_350 = nrow(sclass_350)
n_train350 = round(0.8*n_350)
n_test350 = n_350 - n_train350
```
```{r RMSE~k 350}
k_grid = seq(2, 40, by=1)
rmse_grid350 = foreach(k=k_grid, .combine=c) %do%
{
  out = do(500)*
    {
      train_ind = sample.int(n_350, n_train350, replace=FALSE)
      D_train = sclass_350[train_ind, ]
      D_test = sclass_350[-train_ind, ]
      X_train = data.frame(dplyr::select(D_train, mileage))
      X_test = data.frame(dplyr::select(D_test, mileage))
      y_train = data.frame(dplyr::select(D_train, price))
      y_test = data.frame(dplyr::select(D_test, price))
      # KNN regression
      knn = FNN::knn.reg(train=X_train, test=X_test, y=y_train$price, k=k)
      rmse(y_test, knn$pred)
    }
  mean(out$result)
}
rmse_grid_350 = data.frame(K=k_grid, RMSE=rmse_grid350)
# rmse_grid_350
```

We split the dataset into training and testing randomly(80% training data, 20% test data). Then, repeat the random train-test split for 1000 times for each value of k from 2 to 40 and find the optimal k with the minimum mean RMSE.
The result of the optimal k is shown below.
```{r optimal k 350}
ind_opt1 = which.min(rmse_grid_350$RMSE)
k_opt1 = k_grid[ind_opt1]
p1 = ggplot(data=rmse_grid_350) +
  geom_point(aes(x=K, y=RMSE)) +
  geom_vline(xintercept=k_opt1, color='blue', size=0.5) +
  xlab('k')
p1
```
###### Figure 3: RMSE against k. The optimal k is given by the vertical line.



The value of hyperparameter k in our fitted model:
```{r}
k_opt1
```





#### Fitted KNN Regression Model of S Class 350
```{r}
knn_350 = FNN::knn.reg(train=X_train, test=X_test, y=y_train$price, k=k_opt1)
rmse_min = rmse(y_test, knn_350$pred)
D_test350 = dplyr::select(D_test, mileage, price)
D_test350$y_pred = knn_350$pred
```
```{r plot 350}
colors = c('train'='gray', 'test'='blue', 'predict'='red')
p_350 = ggplot() +
  geom_point(data=sclass_350,aes(x=mileage,y=price,color='train'),size=1) +
  geom_point(data=D_test350,aes(x=mileage,y=y_pred,color='predict'),alpha=0.5,size=1.5) +
  geom_point(data=D_test350,aes(x=mileage,y=price,color='test'),alpha=0.5,size=1) +
  theme_bw() +
  labs(subtitle='Mercedes S class  - 350', 
       caption=paste('Minimum RMSE: ',round(rmse_min),'$')) +
  ggtitle('KNN Predictive Model for Price') +
  xlab('Mileage(km)') + ylab('Price($)') +
  scale_x_continuous(breaks= seq(0, 150000, by=50000), labels=scales::comma) +
  scale_y_continuous(breaks= seq(0, 100000, by=20000), labels=scales::comma) +
  labs(color='Data') +  
  scale_color_manual(values=colors)
p_350  
```

###### Figure 4: Fitted KNN predictive model for price(S Class 350)




#### *Trim: S Class 65AMG*
```{r}
n_65AMG = nrow(sclass_65AMG)
n_train65AMG = round(0.8*n_65AMG)
n_test65AMG = n_65AMG - n_train65AMG

# rmse~k
k_grid = seq(2, 30, by=1)
rmse_grid65AMG = foreach(k=k_grid, .combine=c) %do%
  {
    out = do(500)*
      {
        train_ind = sample.int(n_65AMG, n_train65AMG, replace=FALSE)
        D_train = sclass_65AMG[train_ind, ]
        D_test = sclass_65AMG[-train_ind, ]
        X_train = data.frame(dplyr::select(D_train, mileage))
        X_test = data.frame(dplyr::select(D_test, mileage))
        y_train = data.frame(dplyr::select(D_train, price))
        y_test = data.frame(dplyr::select(D_test, price))
        # KNN regression
        knn = FNN::knn.reg(train=X_train, test=X_test, y=y_train$price, k=k)
        rmse(y_test, knn$pred)
      }
    mean(out$result)
  }
rmse_grid_65AMG = data.frame(K=k_grid, RMSE=rmse_grid65AMG)
```

We split the dataset into training and testing randomly(80% training data, 20% test data). Then, repeat the random train-test split for 1000 times for each value of k from 2 to 30 and find the optimal k with the minimum mean RMSE.
The result of the optimal k is shown below.

```{r}
ind_opt2 = which.min(rmse_grid_65AMG$RMSE)
k_opt2 = k_grid[ind_opt2]
p2 = ggplot(data=rmse_grid_65AMG) +
  geom_point(aes(x=K, y=RMSE)) +
  geom_vline(xintercept=k_opt2, color='blue', size=0.5) +
  xlab('k')
p2
```

###### Figure 5: RMSE against k. The optimal k is given by the vertical line.

The value of hyperparameter k in our fitted model:
```{r}
k_opt2
```



#### Fitted KNN Regression Model of S Class 65AMG
```{r}
# optimal k
knn_65AMG = FNN::knn.reg(train=X_train, test=X_test, y=y_train$price, k=k_opt2)
rmse_min = rmse(y_test, knn_65AMG$pred)
D_test65AMG = dplyr::select(D_test, mileage, price)
D_test65AMG$y_pred = knn_65AMG$pred

# plot
p_65AMG = ggplot() +
  geom_point(data=sclass_65AMG, aes(x=mileage, y=price, color='train'), size=1) +
  geom_point(data=D_test65AMG, aes(x=mileage, y=y_pred, color='predict'), alpha=0.5, size=1.5) +
  geom_point(data=D_test65AMG, aes(x=mileage, y=price, color='test'), alpha=0.5, size=1) +
  theme_bw() +
  ggtitle('KNN Predictive Model for price') +
  labs(subtitle='Mercedes S class - 65AMG', caption=paste('Minimum RMSE: ', round(rmse_min), '$')) +
  xlab('Mileage (km)') + ylab('Price ($)') +
  scale_x_continuous(breaks= seq(0, 150000, by=50000), labels=scales::comma) +
  scale_y_continuous(breaks= seq(0, 250000, by=50000), labels=scales::comma) +
  labs(color='Data') +
  scale_color_manual(values=colors)
p_65AMG
```

###### Figure 6: Fitted KNN predictive model for price(S Class 65AMG)


### Conclusion
The analysis above shows that trim of 65AMG yields a larger optimal value of k in KNN than trim of 350.  From the scatter plots of price against mileage for S Class 350 and 65 AMG(Fig.1 and Fig.2), we can find out that the data points in the trim 350 can be separated into two clusters, while data points in the trim 65AMG is generally more evenly distributed.  Also, the two clusters of data points in trim 350 have overlap on the horizontal(mileage) axis.  That is to say, a comparatively larger k for KNN regression are more likely to average more datapoints in the other cluster, which can lead to a larger RMSE.





## 2.Predicting House Price
     
### Objective
The objective of the report is to present a price-modeling strategiy for taxing authority to predict house price in Saratoga, NY.


### Data
#### *Distribution of House Price*
```{r}
ggplot(data=SaratogaHouses) +
  geom_histogram(aes(x=price), fill='#46ACC9', color='black', binwidth=10000) +
  scale_x_continuous(breaks= seq(0, 800000, by=100000), labels=scales::comma) +
  xlab('House Price($)') + ylab('Count')
```

#### *Variables*
We have 16 variable including the target variable `price`.
```{r, echo=TRUE}
colnames(SaratogaHouses)
```


#### *Correlation between Numerical Variables*
```{r house correlation}
numVar = which(sapply(SaratogaHouses, is.numeric))  
num_df = SaratogaHouses[ , numVar]
cor_num = cor(num_df, use='all.obs')
cor_sorted = data.matrix(sort(cor_num[ , 'price'], decreasing=TRUE))
cor_num = cor_num[rownames(cor_sorted), rownames(cor_sorted)]
col = colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor_num, method="color", col=col(200), type="upper", number.cex=0.4,
         addCoef.col="black", tl.col="gray", diag=FALSE )
```

#### *Price vs. Living Area*
```{r}
ggplot(data=SaratogaHouses, aes(x=livingArea, y=price)) +
  geom_point(color='#46ACC9', size=1) +
  geom_smooth(method = 'lm', se=FALSE, color='blue', size=1) +
  scale_y_continuous(breaks= seq(0, 800000, by=100000), labels=scales::comma) +
  theme_minimal() + xlab('Living Area(sqft.)') + ylab('Price($)')
```

#### *Price vs. Land Value*
```{r}
ggplot(data=SaratogaHouses, aes(x=landValue, y=price)) +
  geom_point(color='#46ACC9', size=1) +
  geom_smooth(method = 'lm', se=FALSE, color='blue', size=1) +
  scale_y_continuous(breaks= seq(0, 800000, by=100000), labels=scales::comma) +
  scale_x_continuous(breaks= seq(0, 400000, by=100000), labels=scales::comma) +
  theme_minimal() + xlab('Land Value($/sqft.)') + ylab('Price($)')
```


It can be shown that the `house price` has strong positive correlation to `Living Area` and `Land Value`. Some variables in the dataset like `rooms`, `bathrooms`, `bedrooms` have strong correlation.



### Method
#### Linear Regression
We fit a simple linear regression model as our baseline.  To improve our linear model, some correlated variables are removed using our correlation matrix and interaction variables are selected using forward selection.

#### KNN Rrgression
KNN regression model is used to predict house price. We find a heuristically optimal number k of nearest neighbors, based on RMSE (Root Mean Square Error)

We compare different models using mean RMSE(Root Mean Square Error) over random train-test split.  



### Result
```{r}
rmse = function(y, yhat) 
{
  sqrt(mean(data.matrix((y-yhat)^2) ))
}
```
#### *Approach 1: Linear Regression*

##### Baseline Linear Model
The baseline linear model `lm_medium` uses a medium size of variables in the dataset.
```{r, echo=TRUE}
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=SaratogaHouses)
```

##### Forward selection
The forward selection consider all 3 way interaction between all variables. This approach can help us find the more important variables and interactions in our linear regression.
```{r, echo=TRUE}
lm0 = lm(price ~ 1, data=SaratogaHouses)
lm_full = lm(price ~ (.)^2, data=SaratogaHouses)
lm_forward = step(lm0, direction='forward', scope=formula(lm_full), trace=0)
```

The selected variables:
```{r, include=TRUE}
names(coef(lm_forward))[-1]
```

##### Best Linear Model
Based on the medium model and forward selection model, we can find a linear model that performs best.
We compare different models over 100 random train-test split.
```{r, include=TRUE}
rmse_vals = do(100)*
  {
    n = nrow(SaratogaHouses)
    n_train = round(0.8*n)
    n_test = n - n_train
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    saratoga_train = SaratogaHouses[train_cases,]
    saratoga_test = SaratogaHouses[test_cases,]
    X_train = model.matrix(~(.-heating-sewer-fuel-waterfront-newConstruction-centralAir-price)^2, data=saratoga_train)[,-1]
    y_train = saratoga_train$price
    X_test = model.matrix(~(.-heating-sewer-fuel-waterfront-newConstruction-centralAir-price)^2, data=saratoga_test)[,-1]
    y_test = saratoga_train$price
    # fit to training set
    # lm_medium
    lm_medium = lm(price ~ lotSize+age+livingArea+pctCollege+bedrooms+fireplaces+ bathrooms+rooms+heating+fuel+centralAir, data=saratoga_train)
    # lm_forward
    lm0 = lm(price ~ 1, data=saratoga_train)
    lm_full = lm(price ~ (.)^2, data=saratoga_train)
    lm_forward = step(lm0, direction='forward', scope=formula(lm_full), trace=0)
    # lm_best
    lm_1 = lm(price ~ lotSize + age + landValue + livingArea + newConstruction + heating + waterfront +fuel*centralAir + bathrooms:livingArea + bedrooms:livingArea + livingArea:heating + landValue:newConstruction, data=saratoga_train)
        
    # predict on test set
    yhat_medium = predict(lm_medium, saratoga_test)
    yhat_1 = predict(lm_1, saratoga_test)
    yhat_fwd = predict(lm_forward, saratoga_test)
    c(rmse(saratoga_test$price, yhat_medium),
      rmse(saratoga_test$price, yhat_fwd),
      rmse(saratoga_test$price, yhat_1)
    )
  }
rmse_grid = data.frame('Model'=c('lm_medium','lm_forward','lm_best'), RMSE_test=colMeans(rmse_vals))
rmse_grid
```

The best linear model `lm_best`
```{r, echo=TRUE}
lm_best = lm(price ~ lotSize + age + landValue + livingArea + newConstruction + heating + waterfront +fuel*centralAir + bathrooms:livingArea + bedrooms:livingArea + livingArea:heating + landValue:newConstruction, data=saratoga_train)
```

The best linear model has mean RMSE:
```{r, include=TRUE}
rmse_grid[3,2]
```




#### *Approach 2: KNN Regression*
```{r}
n = nrow(SaratogaHouses)
n_train = round(0.8*n)
n_test = n - n_train
train_ind = sample.int(n, n_train, replace=FALSE)
D_train = SaratogaHouses[train_ind, ]
D_test = SaratogaHouses[-train_ind, ]
X_train = data.frame(dplyr::select(D_train, lotSize,age,landValue,livingArea,bathrooms,bedrooms,fireplaces))
X_test = data.frame(dplyr::select(D_test, lotSize,age,landValue,livingArea,bathrooms,bedrooms,fireplaces))
y_train = data.frame(dplyr::select(D_train, price))
y_test = data.frame(dplyr::select(D_test, price))
```

```{r}
k_grid = seq(2, 20, by=1)
rmse_grid = foreach(k=k_grid, .combine=c) %do%
  {
    out = do(100)*
      {
        train_ind = sample.int(n, n_train, replace=FALSE)
        D_train = SaratogaHouses[train_ind, ]
        D_test = SaratogaHouses[-train_ind, ]
        X_train = data.frame(dplyr::select(D_train, lotSize,age,landValue,livingArea,bathrooms,bedrooms,fireplaces,pctCollege))
        X_test = data.frame(dplyr::select(D_test, lotSize,age,landValue,livingArea,bathrooms,bedrooms,fireplaces,pctCollege))
        # standardization
        scale_factors = apply(X_train, 2, sd)
        X_train_sc = scale(X_train, scale=scale_factors)
        X_test_sc = scale(X_test, scale=scale_factors)
        
        y_train = data.frame(dplyr::select(D_train, price))
        y_test = data.frame(dplyr::select(D_test, price))
        # KNN regression
        knn = FNN::knn.reg(train=X_train_sc, test=X_test_sc, y=y_train$price, k=k)
        rmse(y_test, knn$pred)
      }
    mean(out$result)
  }
rmse_k = data.frame(K=k_grid, RMSE=rmse_grid)
# rmse_k
```

We split the dataset into training and testing randomly(80% training data, 20% test data). Then, repeat the random train-test split for 500 times for each value of k from 2 to 20 and find the optimal k with the minimum mean RMSE.
The result of the optimal k is shown below.
```{r}
ind_opt = which.min(rmse_k$RMSE)
k_opt = k_grid[ind_opt]
ggplot(data=rmse_k) +
  geom_point(aes(x=K, y=RMSE)) +
  geom_vline(xintercept=k_opt, color='blue', size=0.5)
```
###### figure: RMSE against k. The optimal k is given by the vertical line.



The value of hyperparameter k in our fitted model:
```{r, include=TRUE}
k_opt
```


#### Fitted KNN Regression Model for price
```{r}
knn = FNN::knn.reg(train=X_train_sc, test=X_test_sc, y=y_train$price, k=k_opt)
D_test = dplyr::select(D_test, lotSize,age,landValue,livingArea,bathrooms,bedrooms,fireplaces,pctCollege, price)
D_test$ypred = knn$pred

# plot
colors = c('train'='gray', 'test'='blue', 'predict'='red')
p = ggplot() +
  geom_point(data=SaratogaHouses, aes(x=livingArea, y=price, color='train'), 
             size=1) +
  geom_point(data=D_test, aes(x=livingArea, y=ypred, color='predict'),
             alpha=0.5, size=1.5) +
  geom_point(data=D_test, aes(x=livingArea, y=price, color='test'), 
             alpha=0.5, size=1) +
  theme_bw() +
  ggtitle('KNN Predictive Model for price\n') +
  xlab('Living Area(sqft)') + ylab('Price($)') +
  labs(color='Data') +
  scale_color_manual(values=colors) +
  scale_y_continuous(breaks= seq(0, 1000000, by=200000), labels=scales::comma)
p
```
###### figure: Fitted KNN predictive model for price(S Class 350)


The minimum RMSE of KNN regression:
```{r, include=TRUE}
rmse_k[k_opt,2]
```



### Conclusion
From the result of linear regression and KNN regression, linear regression has a generally lower RMSE, which means that our best linear model has a better performance over KNN regression.
In the house price prediction perspective, linear models may be the better solution. As we can see, we can consider interactions and categorical variables better in linear regression than KNN regression, the extra information of `new construction`, `heating`, etc. help us make better prediction.







## 3.Predicting When Articles Go Viral
##   *regression and classfication approach*
     
### Objective
The data in [online_news.csv](../data/online_news.csv) contains data on 39,797 online rticles published by Mashable during 2013 and 2014.  The target variable is `shares`, i.e. how many times the article was shared on social media.  The other variables are article-level features: things like how long the headline is, how long the article is, how positive or negative the "sentiment" of the article was, and so on.  The full list of features is in [online_news_codes.txt](../data/online_news_codes.txt).  

Mashable is interested in building a model for whether the article goes viral or not.  They judge this on the basis of a cutoff of 1400 shares -- that is, the article is judged to be "viral" if shares > 1400.  Mashable wants to know if there's anything they can learn about how to improve an article's chance of reaching this threshold.   

We first approach this problem from the standpoint of regression.  To assess the performance of your model on a test set, we *threshold* the model's predictions:
- if predicted shares exceeds 1400, predict the article as "viral"
- if predicted shares are 1400 or lower, predict the article as "not viral"

As a second pass, we approach this problem from the standpoint of classification.  That is, define a new variable `viral = ifelse(shares > 1400, 1, 0)` and build a model for directly predicting viral status as the target variable.  



### Data
```{r news normalizaiton}
# normalizatoin
df = data.frame(news)
var = colnames(df)
var_cat = c('data_channel_is_lifestyle','data_channel_is_entertainment','data_channel_is_bus','data_channel_is_socmed','data_channel_is_tech','data_channel_is_world','weekday_is_monday','weekday_is_tuesday','weekday_is_wednesday','weekday_is_thursday','weekday_is_friday','weekday_is_saturday','weekday_is_sunday','is_weekend','shares')
var_num = var[!var %in% var_cat]
scale_factors = apply(df[,var_num], 2, sd)   # sd of the columns
df_sc = data.frame(scale(df[,var_num], scale=scale_factors))
df_sc = cbind(df_sc, df[, var_cat])
```

#### Variables
There are 37 variables in the dataset, including the target variable 'shares'.
```{r, include=TRUE}
var
```

#### Distribution of Shares
```{r plot origin}
## shares distribution
ggplot(data=news) +
  geom_histogram(aes(x=shares), fill='#46ACC9', binwidth=100) +
  ylim(0, 3000) + xlim(0, 10000) +
  xlab('Shares') + ylab('Count')
```


#### Correlation between Variables
```{r news correlation}
corr = cor(df_sc)
names = rownames(corr)
names = names[!names %in% c('self_reference_min_shares','self_reference_max_shares','weekday_is_saturday','weekday_is_sunday','min_positive_polarity','max_positive_polarity','min_negative_polarity','max_negative_polarity','title_sentiment_polarity','weekday_is_monday','weekday_is_tuesday','weekday_is_wednesday','weekday_is_thursday','weekday_is_friday','is_weekend','data_channel_is_lifestyle','data_channel_is_entertainment','data_channel_is_bus','data_channel_is_socmed','data_channel_is_tech','data_channel_is_world')]
cor_news = corr[names, names]
# corr plot
pdf('correlation plot.pdf')
col = colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor_news, method="color", col=col(200), type="upper", number.cex=0.4,
         addCoef.col="black", tl.col="gray", diag=FALSE )
dev.off()
```
###### Figure 1: The coorelation matrix of selected variables.
It can be observed that the *title subjectivity* and *absolute title polarity* are positively correlated, *number of words in the content* and *number of links* and *number of images* are also correlated positively.  Also, *average length of the words in the content* is correlated with *average polarity of positive words* and *average polarity of negative words*.



### Method
#### Regression Approach: Linear Regression
We fit a simple linear regression model with all the given variables to predict shares as our baseline.  To improve our linear model, some correlated variables are removed using our correlation matrix, and lasso regression is also imlplemented.

#### Classfication Approach: Logistic Regression
Logistic regression model is fitted to predict whether the shares are greater than 1400.  Step forward feature selection is implemented to find the best model.

We compare different models using mean RMSE(Root Mean Square Error) over 100 random train-test split.  Confusion matrix is used to evaluate the performance of fitted models.



### Result
```{r news train-test}
n = nrow(df_sc)
n_train = round(0.8*n)
n_test = n-n_train
train_ind = sample.int(n, n_train, replace=FALSE)
D_train = df_sc[train_ind, ]
D_test = df_sc[-train_ind, ]
X_train = model.matrix(~(.-shares), data=D_train)[,-1]
y_train = D_train$shares
X_test = model.matrix(~(.-shares), data=D_test)[,-1]
y_test = D_test$shares 
```

#### *Approach 1: Regression*
The baseline linear model `lm_full` uses all variables in the dataset.
```{r, echo=TRUE}
lm_full = lm(shares~(.), data=D_train)
```

We can notice that `max_negative_polarity`, `min_negative_polarity`, `avg_negative_polarity` and `max_positive_polarity`, `min_positive_polarity`, `avg_positive_polarity` and `self_reference_max_shares`, `self_reference_min_shares`, `self_reference_avg_shares` are correlated, so we only keep `avg_negative_polarity`, `avg_positive_polarity` and `self_reference_avg_shares` in our model `lm_1`.
```{r, echo=TRUE}
lm_1 = lm(shares~(.-max_negative_polarity-max_positive_polarity-self_reference_max_shares-self_reference_min_shares-is_weekend-min_positive_polarity-min_negative_polarity), data=D_train)
```


##### Lasso Regressions 
We remove the same correlated variables mentioned above and then apply the lasso regression.
```{r}
# feature selection
var = colnames(D_train)
var_out = c('self_reference_min_shares','self_reference_max_shares','is_weekend','min_positive_polarity','max_positive_polarity','min_negative_polarity','max_negative_polarity')
var_in = var[!var %in% var_out]
D_train = data.frame(D_train[, var_in])
D_test = data.frame(D_test[, var_in])
X_train = model.matrix(~(.-shares), data=D_train)[,-1]
y_train = D_train$shares
X_test = model.matrix(~(.-shares), data=D_test)[,-1]
y_test = D_test$shares
```
```{r news lasso, echo=TRUE}
lm_lasso = glmnet(X_train, y_train, alpha=1, standardize=FALSE)
```

Comparison of the mean RMSE in the test set over 100 random train-test split:
```{r news 100}
rmse_mean = do(100)*{
  n = nrow(df_sc)
  n_train = round(0.8*n)
  n_test = n-n_train
  train_ind = sample.int(n, n_train, replace=FALSE)
  D_train = df_sc[train_ind, ]
  D_test = df_sc[-train_ind, ]
  X_train = model.matrix(~(.-shares), data=D_train)[,-1]
  y_train = D_train$shares
  X_test = model.matrix(~(.-shares), data=D_test)[,-1]
  y_test = D_test$shares
  
  # simple lm
  lm_full = lm(shares~(.), data=D_train)
  full = rmse(y_test, predict(lm_full, D_test))
  
  # lm_1: remove correlated variables
  lm_1 = lm(shares~(.-max_negative_polarity-max_positive_polarity-self_reference_max_shares-self_reference_min_shares-is_weekend-min_positive_polarity-min_negative_polarity), data=D_train)
  part = rmse(y_test, predict(lm_1, D_test))
  
  # lasso
  X = model.matrix(~(.-max_negative_polarity-max_positive_polarity-self_reference_max_shares-self_reference_min_shares-is_weekend-min_positive_polarity-min_negative_polarity), data=data.frame(X_train))[,-1]
  X_t = model.matrix(~(.-max_negative_polarity-max_positive_polarity-self_reference_max_shares-self_reference_min_shares-is_weekend-min_positive_polarity-min_negative_polarity), data=data.frame(X_test))[,-1]
  
  lm_lasso = glmnet(X, y_train, alpha=1, standardize=FALSE)
  yhat_lasso = predict(lm_lasso, X_t)
  lasso = rmse(y_test, yhat_lasso)
  
  c(full, part, lasso)
}
rmse_grid = data.frame('Model'=c('simple lm','lm_1','lasso'), RMSE_test=colMeans(rmse_mean))
rmse_grid
```


#### Confusion Matrix
We choose `lm_1` as our fitted model, since it has the best out-of-sample perfomance.

The confusion matrix over 100 random train-test split:
```{r}
confusion = do(100)*{
  train_ind = sample.int(n, n_train, replace=FALSE)
  D_test = df_sc[-train_ind, ]
  D_test$predict = predict(lm_1, D_test)
  D_test$viral = ifelse(D_test$shares>1400, 1, 0)
  D_test$viralhat = ifelse(D_test$predict>1400, 1, 0)
  confusion_test = table(y=D_test$viral, yhat=D_test$viralhat)
  cell_1 = confusion_test[1,1]
  cell_2 = confusion_test[1,2]
  cell_3 = confusion_test[2,1]
  cell_4 = confusion_test[2,2]
  c(cell_1, cell_2, cell_3, cell_4)
}
cell_mean = colMeans(confusion)
confusion_test[1,1] = cell_mean[1]
confusion_test[1,2] = cell_mean[2]
confusion_test[2,1] = cell_mean[3]
confusion_test[2,2] = cell_mean[4]
confusion_test
```
Average overall error rate:
```{r}
1-sum(diag(confusion_test))/sum(confusion_test)
```
Average true positive rate:
```{r}
confusion_test[2,2]/sum(confusion_test[2,])
```
Average false positive rate:
```{r}
confusion_test[1,2]/sum(confusion_test[1,])
```


The null model (predict all news go viral)
Average overall error rate:
```{r}
df$viral = ifelse(df$shares>1400, 1, 0)
1-mean(df$viral)
```
Average true positive rate:
```{r}
1
```
Average false positive rate:
```{r}
1
```

Our best linear regression model only does slightly better than the *null* model and the false positive rate is unreasonably high.




#### *Approach 2: Classification*
```{r}
news = read.csv('C:/Users/thinkpad/Desktop/FOCUS/UT_Course/SDS 323/exercise_2/data/online_news.csv')
news = dplyr::select(news, -url)
news$viral = ifelse(news$shares>1400, 1, 0)
df = data.frame(dplyr::select(news, -shares))
var = colnames(df)
var_cat = c('data_channel_is_lifestyle','data_channel_is_entertainment','data_channel_is_bus','data_channel_is_socmed','data_channel_is_tech','data_channel_is_world','weekday_is_monday','weekday_is_tuesday','weekday_is_wednesday','weekday_is_thursday','weekday_is_friday','weekday_is_saturday','weekday_is_sunday','is_weekend','viral')
var_num = var[!var %in% var_cat]

# normalization
scale_factors = apply(df[,var_num], 2, sd)   # sd of the columns
df_sc = data.frame(scale(df[,var_num], scale=scale_factors))
df_sc = cbind(df_sc, df[, var_cat])
# train-test
train_ind = sample.int(n, n_train, replace=FALSE)
X_train = df_sc[train_ind, ]
X_test = df_sc[-train_ind, ]
y_train = X_train$viral
y_test = X_test$viral
```

#### Logistic Regression
The fitted lasso regression.
```{r, echo=TRUE}
logit_best = glm(viral~(. - self_reference_min_shares - self_reference_max_shares - is_weekend - min_positive_polarity - max_positive_polarity - min_negative_polarity - max_negative_polarity)^2, data=X_train, family='binomial')
```



#### Confusion Matrix

The confusion matrix over 100 random train-test split:
```{r}
confusion = do(100)*{
  train_ind = sample.int(n, n_train, replace=FALSE)
  X_test = df_sc[-train_ind, ]
  X_test$predict = ifelse(predict(logit_best, X_test)>0.5, 1, 0)
  confusion_test = table(y=X_test$viral, yhat=X_test$predict)
  cell_1 = confusion_test[1,1]
  cell_2 = confusion_test[1,2]
  cell_3 = confusion_test[2,1]
  cell_4 = confusion_test[2,2]
  c(cell_1, cell_2, cell_3, cell_4)
}
cell_mean = colMeans(confusion)
confusion_test[1,1] = cell_mean[1]
confusion_test[1,2] = cell_mean[2]
confusion_test[2,1] = cell_mean[3]
confusion_test[2,2] = cell_mean[4]
confusion_test
```
Average overall error rate:
```{r}
1-sum(diag(confusion_test))/sum(confusion_test)
```
Average true positive rate:
```{r}
confusion_test[2,2]/sum(confusion_test[2,])
```
Average false positive rate:
```{r}
confusion_test[1,2]/sum(confusion_test[1,])
```


The null model (predict all news go viral)
Average overall error rate:
```{r}
1-mean(df$viral)
```
Average true positive rate:
```{r}
1
```
Average false positive rate:
```{r}
1
```




### Conclusion
Our best logistic regression model has a lower overall error(< 40%)
rate than the null model and our linear regression model in classification.
The linear model has a rather high true positive and false positive rate. This means that our linear model performs poorly when the news does not go viral.
The logistic regrssion model has a much lower false positive rate than linear model, but performs worse than linear model when the news actually goes viral.



